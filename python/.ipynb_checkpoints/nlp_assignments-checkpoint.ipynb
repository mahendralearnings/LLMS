{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd3292-d1ee-4584-95c4-1ddf1155e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize a sentence into words.\n",
    "x = 'India is great'\n",
    "xx = x.split()\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f4391-ea3b-4741-b2c1-efa855b0c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove punctuation from a text.\n",
    "x = 'India is great!'\n",
    "xx = ''.join(char if char.isalnum() or char.isspace() else '' for char in x)\n",
    "print(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae48cc-aaee-436e-a5cf-d493d5ebc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Convert text to lowercase.\n",
    "x = 'INDIA IS GREAT'\n",
    "print(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d44b3-069c-4c4d-9f0d-6b13dc16e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Count the frequency of words in a text.\n",
    "from collections import Counter\n",
    "x = 'india is great and india is populated country'\n",
    "c = x.split()\n",
    "xx = Counter(c)\n",
    "print(xx)\n",
    "\n",
    "d = {}\n",
    "for i in c:\n",
    "    if i in d:\n",
    "        d[i] = d[i]+1\n",
    "    else:\n",
    "        d[i] = 1\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344f224-6656-48f9-8f14-282a637cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove stopwords from a text.\n",
    "x = 'India is great! and it is very beautiful country'\n",
    "def remove_stopwords(text, stopwords):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    \n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Test the function\n",
    "text = \"This is a sample sentence with some stopwords such as the, is, and, for testing purposes.\"\n",
    "stopwords = [\"the\", \"is\", \"and\", \"such\", \"as\", \"with\", \"for\"]\n",
    "filtered_text = remove_stopwords(text, stopwords)\n",
    "filtered_text1 = remove_stopwords(x, stopwords)\n",
    "print(\"Text without stopwords:\", filtered_text)\n",
    "print(\"Text without stopwords:\", filtered_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6764e65d-f2ad-4494-b9ba-99ced4c7ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Stemmed text (Porter): i am ride bicycl and swim in the river\n",
      "Stemmed text (Snowball): i am ride bicycl and swim in the river\n"
     ]
    }
   ],
   "source": [
    "# 6. Stem words in a text using stemming algorithms (e.g., Porter, Snowball).\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "def stem_words(text, stemmer='porter'):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Initialize stemmer based on the specified algorithm\n",
    "    if stemmer.lower() == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer.lower() == 'snowball':\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stemmer specified. Choose 'porter' or 'snowball'.\")\n",
    "    \n",
    "    # Stem each word in the list of words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the stemmed words back into a sentence\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return stemmed_text\n",
    "\n",
    "# Test the function\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "stemmed_text_porter = stem_words(text, stemmer='porter')\n",
    "stemmed_text_snowball = stem_words(text, stemmer='snowball')\n",
    "\n",
    "print(\"Stemmed text (Porter):\", stemmed_text_porter)\n",
    "print(\"Stemmed text (Snowball):\", stemmed_text_snowball)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9763291-0a79-47e7-afc1-c5ba9325a5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sivav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sivav\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text: I am riding bicycle and swimming in the river\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Lemmatize words in a text using lemmatization.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize each word in the list of words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmatized words back into a sentence\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "# Test the function\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "lemmatized_text = lemmatize_words(text)\n",
    "print(\"Lemmatized text:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "165fbd06-53b8-444c-b41d-08eae01c3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycles\n",
      "['riding', 'bicycles', 'swimming']\n",
      "['I', 'am', 'and', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Find the longest word in a text.\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "tokens = text.split()\n",
    "x = 0\n",
    "lon_word = ''\n",
    "for i in tokens:\n",
    "    if len(i) > x:\n",
    "        lon_word = i\n",
    "        x = len(i)\n",
    "    else:\n",
    "        pass\n",
    "print(lon_word)\n",
    "\n",
    "#find words whose length is 5 or more\n",
    "x = [i for i in text.split() if len(i) > 5]\n",
    "xx = [i for i in text.split() if len(i) <4 ]\n",
    "print(x)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32aa5449-d3c5-4d20-bd39-aff5467856eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Find the average word length in a text.\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "xx = sum((len(i) for i in text.split()))\n",
    "c = len(text.split())\n",
    "x = xx/c\n",
    "print(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c679ed23-2cf1-4b36-bdbf-2359204b6a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical diversity: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. Calculate the lexical diversity of a text.\n",
    "def lexical_diversity(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the total number of words\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Calculate the total number of unique words\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    # Calculate the lexical diversity\n",
    "    if total_words > 0:\n",
    "        diversity = unique_words / total_words\n",
    "    else:\n",
    "        diversity = 0\n",
    "    \n",
    "    return diversity\n",
    "\n",
    "# Test the function\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "diversity = lexical_diversity(text)\n",
    "print(\"Lexical diversity:\", diversity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6eef3cd0-7e38-4298-bbb3-d35c03c452d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-grams: [('I', 'am'), ('am', 'riding'), ('riding', 'bicycles'), ('bicycles', 'and'), ('and', 'swimming'), ('swimming', 'in'), ('in', 'the'), ('the', 'river')]\n",
      "Tri-grams: [('I', 'am', 'riding'), ('am', 'riding', 'bicycles'), ('riding', 'bicycles', 'and'), ('bicycles', 'and', 'swimming'), ('and', 'swimming', 'in'), ('swimming', 'in', 'the'), ('in', 'the', 'river')]\n",
      "('I', 'am')\n",
      "('am', 'riding')\n",
      "('riding', 'bicycles')\n",
      "('bicycles', 'and')\n",
      "('and', 'swimming')\n",
      "('swimming', 'in')\n",
      "('in', 'the')\n",
      "('the', 'river')\n",
      "Bi-grams: [('I', 'am'), ('am', 'riding'), ('riding', 'bicycles'), ('bicycles', 'and'), ('and', 'swimming'), ('swimming', 'in'), ('in', 'the'), ('the', 'river')]\n",
      "('I', 'am', 'riding')\n",
      "('am', 'riding', 'bicycles')\n",
      "('riding', 'bicycles', 'and')\n",
      "('bicycles', 'and', 'swimming')\n",
      "('and', 'swimming', 'in')\n",
      "('swimming', 'in', 'the')\n",
      "('in', 'the', 'river')\n",
      "Tri-grams: [('I', 'am', 'riding'), ('am', 'riding', 'bicycles'), ('riding', 'bicycles', 'and'), ('bicycles', 'and', 'swimming'), ('and', 'swimming', 'in'), ('swimming', 'in', 'the'), ('in', 'the', 'river')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'am', 'riding'),\n",
       " ('am', 'riding', 'bicycles'),\n",
       " ('riding', 'bicycles', 'and'),\n",
       " ('bicycles', 'and', 'swimming'),\n",
       " ('and', 'swimming', 'in'),\n",
       " ('swimming', 'in', 'the'),\n",
       " ('in', 'the', 'river')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 11. Generate n-grams (bi-grams, tri-grams) from a text.\n",
    "def generate_ngrams(text, n):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Generate n-grams\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Test the function\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "bi_grams = generate_ngrams(text, 2)\n",
    "tri_grams = generate_ngrams(text, 3)\n",
    "\n",
    "print(\"Bi-grams:\", bi_grams)\n",
    "print(\"Tri-grams:\", tri_grams)\n",
    "\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Initialize an empty list to store the n-grams\n",
    "    ngrams = []\n",
    "    \n",
    "    # Create n-grams by iterating over the words with a sliding window\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = tuple(words[i:i+n])  # Create a tuple for the n-gram\n",
    "        print(ngram)\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Test the function with bi-grams (n=2)\n",
    "text = \"I am riding bicycles and swimming in the river\"\n",
    "bi_grams = generate_ngrams(text, n=2)\n",
    "print(\"Bi-grams:\", bi_grams)\n",
    "\n",
    "# Test the function with tri-grams (n=3)\n",
    "tri_grams = generate_ngrams(text, n=3)\n",
    "print(\"Tri-grams:\", tri_grams)\n",
    "\n",
    "x = text.split()\n",
    "xx = []\n",
    "n = 3\n",
    "for i in range(len(x)-n+1):\n",
    "    c = tuple(x[i:i+n])\n",
    "    xx.append(c)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a264955-03be-4f46-8a12-c8f20411c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.4/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.6/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.9/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.2/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.4/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 6.8 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.0/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.3/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Named Entities: [('Apple', 'ORG'), ('Cupertino', 'GPE'), ('California', 'GPE'), ('Steve Jobs', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "#finding NER from text\n",
    "def ner(text):\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the text with the NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities and their labels\n",
    "    named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    \n",
    "    return named_entities\n",
    "\n",
    "# Test the function\n",
    "text = \"Apple is headquartered in Cupertino, California, and was founded by Steve Jobs.\"\n",
    "named_entities = ner(text)\n",
    "print(\"Named Entities:\", named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21943bff-fd3b-4491-a46e-249d78eb2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in@in.com']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 14. Extract email addresses from a text.\n",
    "x = 'Hello my email id is in@in.com'\n",
    "import re\n",
    "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    \n",
    "# Find all matches of email addresses in the sentence\n",
    "emails = re.findall(pattern, x)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7e518-3201-4fb9-ad86-7932832fa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15. Extract URLs from a text.\n",
    "# 16. Detect sentiment (positive, negative, neutral) of a sentence.\n",
    "# 17. Identify the language of a text.\n",
    "# 18. Find collocations (frequent bigrams) in a text.\n",
    "# 19. Perform text classification using machine learning algorithms.\n",
    "# 20. Generate word clouds from a text.\n",
    "# 21. Perform topic modeling (e.g., LDA) on a collection of texts.\n",
    "# 22. Calculate text similarity using cosine similarity.\n",
    "# 23. Translate text from one language to another.\n",
    "# 24. Spell check and correct typos in a text.\n",
    "# 25. Identify gender from given names.\n",
    "# 26. Identify the author of a text using stylometry.\n",
    "# 27. Implement a chatbot using rule-based or machine learning-based approaches.\n",
    "# 28. Generate text summaries using extractive or abstractive methods.\n",
    "# 29. Perform named entity disambiguation in a text.\n",
    "# 30. Extract keywords or keyphrases from a text.\n",
    "# 31. Analyze sentiment of product reviews.\n",
    "# 32. Perform document clustering to group similar texts.\n",
    "# 33. Identify fake news or misinformation in texts.\n",
    "# 34. Perform text classification for spam detection.\n",
    "# 35. Analyze sentiment of tweets or social media posts.\n",
    "# 36. Implement text generation using Markov chains.\n",
    "# 37. Analyze sentiment of movie reviews.\n",
    "# 38. Perform sentiment analysis of customer feedback.\n",
    "# 39. Perform sentiment analysis of news articles.\n",
    "# 40. Perform sentiment analysis of blog posts.\n",
    "# 41. Perform sentiment analysis of product descriptions.\n",
    "# 42. Perform text classification for sentiment analysis on IMDb movie reviews.\n",
    "# 43. Implement a text-based recommender system.\n",
    "# 44. Analyze sentiment of Amazon product reviews.\n",
    "# 45. Identify sarcasm or irony in texts.\n",
    "# 46. Analyze sentiment of Yelp restaurant reviews.\n",
    "# 47. Perform topic modeling on Reddit comments.\n",
    "# 48. Analyze sentiment of airline reviews.\n",
    "# 49. Analyze sentiment of hotel reviews.\n",
    "# 50. Implement a text-based sentiment analysis web application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
