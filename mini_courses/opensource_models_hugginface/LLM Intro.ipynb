{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b947c78e",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Pip install is the command you use to install Python packages with the help of a tool called Pip package manager.\n",
    "<br><br>Installing LangChain package\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8236fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.13\n",
      "  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain==0.1.13) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.13)\n",
      "  Downloading SQLAlchemy-2.0.28-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain==0.1.13) (3.9.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.13)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.13)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.29 (from langchain==0.1.13)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain==0.1.13)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.13)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.13)\n",
      "  Downloading langsmith-0.1.31-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain==0.1.13) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain==0.1.13) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain==0.1.13) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.13)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.13)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain==0.1.13) (4.2.0)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain==0.1.13)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13)\n",
      "  Downloading orjson-3.9.15-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     -------------------------------- ------- 41.0/50.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 50.7/50.7 kB 518.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.1.13)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain==0.1.13) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.13)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
      "   ---------------------------------------- 0.0/810.5 kB ? eta -:--:--\n",
      "   -------------------- ------------------ 430.1/810.5 kB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 778.2/810.5 kB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 778.2/810.5 kB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 778.2/810.5 kB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/810.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 810.5/810.5 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/1.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/1.8 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/1.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.1 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/269.1 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/269.1 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/269.1 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 204.8/269.1 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 256.0/269.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.1 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 269.1/269.1 kB 789.2 kB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.6 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 61.4/71.6 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 61.4/71.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.6/71.6 kB 563.4 kB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.28-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/2.1 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 962.7 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.1 MB 955.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/2.1 MB 982.5 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/2.1 MB 982.5 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 964.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 964.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.1 MB 848.0 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/2.1 MB 888.5 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.7/2.1 MB 901.1 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.7/2.1 MB 930.9 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  286.7/292.8 kB 18.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- 292.8/292.8 kB 905.7 kB/s eta 0:00:00\n",
      "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 41.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 416.2 kB/s eta 0:00:00\n",
      "Downloading orjson-3.9.15-cp311-none-win_amd64.whl (136 kB)\n",
      "   ---------------------------------------- 0.0/136.0 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/136.0 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  133.1/136.0 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  133.1/136.0 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  133.1/136.0 kB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- 136.0/136.0 kB 574.4 kB/s eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 30.7/53.0 kB ? eta -:--:--\n",
      "   -------------------------------------- - 51.2/53.0 kB 650.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 51.2/53.0 kB 650.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 53.0/53.0 kB 274.2 kB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, packaging, orjson, mypy-extensions, jsonpointer, greenlet, typing-inspect, SQLAlchemy, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed SQLAlchemy-2.0.28 dataclasses-json-0.6.4 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 tenacity-8.2.3 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d1a68cc-4373-4e07-94b0-6e2bc5e3771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use Proprietary LLM from - OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e925c",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Installing Openai package, which includes the classes that we can use to communicate with Openai services\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb5a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.14.2\n",
      "  Downloading openai-1.14.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai==1.14.2) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.14.2)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.14.2)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai==1.14.2) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai==1.14.2) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai==1.14.2) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai==1.14.2) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.14.2) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.14.2) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.14.2)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.2)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tqdm>4->openai==1.14.2) (0.4.6)\n",
      "Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
      "   ---------------------------------------- 0.0/262.4 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 174.1/262.4 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/262.4 kB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 262.4/262.4 kB 672.5 kB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 71.7/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 837.8 kB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.8 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/77.8 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/77.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.8/77.8 kB 866.8 kB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 510.8 kB/s eta 0:00:00\n",
      "Installing collected packages: h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.14.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274e9c5",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Imports the Python built-in module called \"os.\"\n",
    "<br>This module provides a way to interact with the operating system, such as accessing environment variables, working with files and directories, executing shell commands, etc\n",
    "<br><br>\n",
    "The environ attribute is a dictionary-like object that contains the environment variables of the current operating system session\n",
    "<br><br>\n",
    "By accessing os.environ, you can retrieve and manipulate environment variables within your Python program. For example, you can retrieve the value of a specific environment variable using the syntax os.environ['VARIABLE_NAME'], where \"VARIABLE_NAME\" is the name of the environment variable you want to access.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d278e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef06cb",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "LangChain has built a Wrapper around OpenAI APIs, using which we can get access to all the services OpenAI provides.\n",
    "<br>\n",
    "The code snippet below imports a specific class called 'OpenAI'(Wrapper around OpenAI large language models) from the 'llms' module of the 'langchain' library.\n",
    "\n",
    "<br>https://python.langchain.com/en/latest/_modules/langchain/llms/openai.html\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db64eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The candidate selected for download or install is a yanked version: 'langchain-openai' candidate (version 0.1.0 at https://files.pythonhosted.org/packages/6d/f4/bea64066e93a4980f0d8352af733f950ff0eea98cca5000a4ca1ff2ae2b8/langchain_openai-0.1.0-py3-none-any.whl (from https://pypi.org/simple/langchain-openai/) (requires-python:<4.0,>=3.8.1))\n",
      "Reason for being yanked: Contained a regression that prevented passing ToolMessage in the input to ChatOpenAI, fixed in 0.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai==0.1.0\n",
      "  Downloading langchain_openai-0.1.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-openai==0.1.0) (0.1.33)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-openai==0.1.0) (1.14.2)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain-openai==0.1.0)\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.1.31)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.4.6)\n",
      "Downloading langchain_openai-0.1.0-py3-none-any.whl (32 kB)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/798.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 41.0/798.7 kB 653.6 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 153.6/798.7 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 399.4/798.7 kB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 675.8/798.7 kB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 798.7/798.7 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, langchain-openai\n",
      "Successfully installed langchain-openai-0.1.0 tiktoken-0.6.0\n"
     ]
    }
   ],
   "source": [
    "#As Langchain team has been working aggresively on improving the tool, we can see a lot of changes happening every weeek,\n",
    "#As a part of it, the below import has been depreciated\n",
    "#from langchain.llms import OpenAI\n",
    "\n",
    "#First we'll need to import the below LangChain - OpenAI integration package and then import it please, if not installed already\n",
    "\n",
    "!pip install langchain-openai==0.1.0\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d124f",
   "metadata": {},
   "source": [
    "<font color='green'>Here we are instantiating a language model object called OpenAI, for our natural language processing tasks.\n",
    "<br><br>\n",
    "The parameter model_name is provided with the value \"text-davinci-003\" which is a specific version or variant of a language model (examples - text-davinci-003, code-davinci-002, gpt-3.5-turbo, text-ada-001 and more).\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3be989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'text-davinci-003' model is depreciated now, so we are using the openai's reccomended model\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "llm = OpenAI(model_name=\"davinci-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c7904",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Here language model is represented by the object \"llm,\" which is being utilized to generate a completion or response based on a specific query. \n",
    "<br><br>\n",
    "The query, stored in the \"our_query\" variable is bieng passed to the model through llm object.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e267f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m our_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the currency of India?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Last week langchain has recommended to use invoke function for the below please :)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(our_query)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    249\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    250\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    251\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    252\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    253\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    254\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    255\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    260\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m         )\n\u001b[0;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m         )\n\u001b[0;32m    747\u001b[0m     ]\n\u001b[1;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    749\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    750\u001b[0m     )\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 593\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    594\u001b[0m                 prompts,\n\u001b[0;32m    595\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    597\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    599\u001b[0m             )\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    602\u001b[0m         )\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\langchain_openai\\llms\\base.py:368\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    353\u001b[0m         {\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m         }\n\u001b[0;32m    366\u001b[0m     )\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(prompt\u001b[38;5;241m=\u001b[39m_prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\resources\\completions.py:516\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    518\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    519\u001b[0m             {\n\u001b[0;32m    520\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    521\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m    522\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_of,\n\u001b[0;32m    523\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho\u001b[39m\u001b[38;5;124m\"\u001b[39m: echo,\n\u001b[0;32m    524\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    525\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    526\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    527\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    528\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    529\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    530\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    531\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    532\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    533\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[0;32m    534\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    535\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    536\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    537\u001b[0m             },\n\u001b[0;32m    538\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    539\u001b[0m         ),\n\u001b[0;32m    540\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    541\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    542\u001b[0m         ),\n\u001b[0;32m    543\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mCompletion,\n\u001b[0;32m    544\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[Completion],\n\u001b[0;32m    546\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1207\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    898\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    899\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    902\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    903\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    974\u001b[0m         options,\n\u001b[0;32m    975\u001b[0m         cast_to,\n\u001b[0;32m    976\u001b[0m         retries,\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1027\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    974\u001b[0m         options,\n\u001b[0;32m    975\u001b[0m         cast_to,\n\u001b[0;32m    976\u001b[0m         retries,\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1027\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\Lib\\site-packages\\openai\\_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    996\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "our_query = \"What is the currency of India?\"\n",
    "\n",
    "#Last week langchain has recommended to use invoke function for the below please :)\n",
    "completion = llm.invoke(our_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f1b87e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completion' is not defined"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2085724",
   "metadata": {},
   "source": [
    "## Let's use open-source LLM hosted on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4f9984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.21.4\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub==0.21.4) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.21.4) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivav\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (2024.2.2)\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/346.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/346.4 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/346.4 kB 435.7 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 41.0/346.4 kB 393.8 kB/s eta 0:00:01\n",
      "   ------------------ --------------------- 163.8/346.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/346.4 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- 346.4/346.4 kB 597.1 kB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed huggingface-hub-0.21.4\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub==0.21.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18ba8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9548eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import HuggingFaceHub\n",
    "\n",
    "#The above have been updated recently, so going forward we have to use the below :)\n",
    "\n",
    "from langchain.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "099a0220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\sivav\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\")\n",
    "\n",
    "#The above 'HuggingFaceHub' class has been depreciated, so please use the below class'HuggingFaceEndpoint' \n",
    "#and the below mentioned model outperforms most of the available open source LLMs\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\") # Model link : https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3beee630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "our_query = \"What is the currency of India?\"\n",
    "\n",
    "#Last week langchain has recommended to use invoke function for the below please :)\n",
    "completion = llm.invoke(our_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259e2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The currency of India is the Indian Rupee. It is represented by the symbol  and the ISO code INR. One rupee is subdivided into 100 paise.\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ec99d-9d5a-4352-9ac1-93ba0d6fdf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
